{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "üîπ 1. Initialization Function (__init__)\n",
        "def __init__(self, lr=0.01, n_iters=1000):\n",
        "    self.lr = lr\n",
        "    self.n_iters = n_iters\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "\n",
        "Explanation:\n",
        "\n",
        "lr (learning rate):\n",
        "Determines how much to adjust the weights after each iteration.\n",
        "A small value = slow learning; a large value = unstable learning.\n",
        "\n",
        "n_iters:\n",
        "Number of training iterations (epochs).\n",
        "\n",
        "weights and bias:\n",
        "Model parameters that will be learned during training.\n",
        "Initially set to None, and then later initialized as zeros inside fit().\n",
        "\n",
        "üîπ 2. Training Function (fit())\n",
        "def fit(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "    self.weights = np.zeros(n_features)\n",
        "    self.bias = 0\n",
        "\n",
        "\n",
        "X ‚Üí input data (matrix of features).\n",
        "\n",
        "y ‚Üí actual labels (0 or 1).\n",
        "\n",
        "n_samples ‚Üí number of rows (data points).\n",
        "\n",
        "n_features ‚Üí number of columns (features).\n",
        "\n",
        "At first, we set:\n",
        "\n",
        "All weights = 0\n",
        "\n",
        "Bias = 0\n",
        "\n",
        "üîπ 3. The Training Loop (Gradient Descent)\n",
        "for _ in range(self.n_iters):\n",
        "    linear_model = np.dot(X, self.weights) + self.bias\n",
        "    y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "Step 1: Linear Model\n",
        "\n",
        "We calculate the raw prediction:\n",
        "\n",
        "ùëß\n",
        "=\n",
        "ùëã\n",
        "‚ãÖ\n",
        "ùë§\n",
        "+\n",
        "ùëè\n",
        "z=X‚ãÖw+b\n",
        "Step 2: Apply Sigmoid Function\n",
        "\n",
        "We pass z through a sigmoid function to convert it into probabilities between 0 and 1:\n",
        "\n",
        "ùúé\n",
        "(\n",
        "ùëß\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ùëí\n",
        "‚àí\n",
        "ùëß\n",
        "œÉ(z)=\n",
        "1+e\n",
        "‚àíz\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "So y_predicted gives the probability that each sample belongs to class 1.\n",
        "\n",
        "üîπ 4. Compute Gradients\n",
        "dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "\n",
        "We calculate how much each parameter (weight and bias) contributes to the total error.\n",
        "These are the gradients ‚Äî the direction of change needed to reduce the error.\n",
        "\n",
        "dw (gradient of weights):\n",
        "Measures how much to change each weight.\n",
        "\n",
        "db (gradient of bias):\n",
        "Measures how much to change the bias.\n",
        "\n",
        "üîπ 5. Update Parameters\n",
        "self.weights -= self.lr * dw\n",
        "self.bias -= self.lr * db\n",
        "\n",
        "\n",
        "This is the Gradient Descent update rule:\n",
        "\n",
        "Move the weights and bias opposite to the gradient direction (to minimize error).\n",
        "\n",
        "Multiply by the learning rate to control step size.\n",
        "\n",
        "The loop repeats for all iterations ‚Äî the model ‚Äúlearns‚Äù by continuously reducing the prediction error.\n",
        "\n",
        "üîπ 6. Predict Function (predict())\n",
        "def predict(self, X):\n",
        "    linear_model = np.dot(X, self.weights) + self.bias\n",
        "    y_predicted = self._sigmoid(linear_model)\n",
        "    y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "    return np.array(y_predicted_cls)\n",
        "\n",
        "\n",
        "Here the model makes predictions on new data.\n",
        "\n",
        "Compute linear model ‚Üí np.dot(X, weights) + bias\n",
        "\n",
        "Convert to probabilities with sigmoid.\n",
        "\n",
        "Apply a threshold of 0.5:\n",
        "\n",
        "If probability > 0.5 ‚Üí predict 1\n",
        "\n",
        "Else ‚Üí predict 0\n",
        "\n",
        "üîπ 7. Sigmoid Function\n",
        "def _sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "This function squashes any real number into the range (0, 1).\n",
        "It‚Äôs what makes Logistic Regression suitable for binary classification.\n",
        "\n",
        "‚úÖ Summary\n",
        "Step\tDescription\n",
        "1\tInitialize weights & bias\n",
        "2\tCompute linear combination (X¬∑w + b)\n",
        "3\tApply sigmoid to get probabilities\n",
        "4\tCompute gradients\n",
        "5\tUpdate weights & bias (gradient descent)\n",
        "6\tRepeat until convergence\n",
        "7\tPredict using learned parameters"
      ],
      "metadata": {
        "id": "PQw5TFJC6_Mb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qs0c8n5j2Hh2"
      },
      "outputs": [],
      "source": [
        " import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.01, n_iters=1000):\n",
        "        # Initialize hyperparameters\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize parameters\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Gradient Descent\n",
        "        for _ in range(self.n_iters):\n",
        "            # Compute linear model\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            # Apply sigmoid function to get probabilities\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict probabilities\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._sigmoid(linear_model)\n",
        "        # Convert probabilities to 0 or 1 (threshold = 0.5)\n",
        "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "        return np.array(y_predicted_cls)\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        # Sigmoid activation function\n",
        "        return 1 / (1 + np.exp(-x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîç Explanation\n",
        "\n",
        "Dataset (X, y):\n",
        "We created a small binary dataset ‚Äî the first three points belong to class 0, and the last three to class 1.\n",
        "Each point has two features.\n",
        "\n",
        "Model Initialization:\n",
        "lr=0.1 ‚Üí the learning rate (how fast the model updates its weights).\n",
        "n_iters=1000 ‚Üí the number of training iterations.\n",
        "\n",
        "Training (fit):\n",
        "The model learns to find a decision boundary that separates class 0 and class 1.\n",
        "\n",
        "Prediction:\n",
        "We test the model on two new samples [[2, 3], [5, 6]].\n",
        "The model returns class labels (0 or 1) based on what it has learned.\n"
      ],
      "metadata": {
        "id": "zA7GaS_27UK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate simple binary dataset\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [2, 3],\n",
        "    [3, 4],\n",
        "    [4, 5],\n",
        "    [5, 6],\n",
        "    [6, 7]\n",
        "])\n",
        "y = np.array([0, 0, 0, 1, 1, 1])  # Binary labels\n",
        "\n",
        "# Initialize and train the model\n",
        "model = LogisticRegression(lr=0.1, n_iters=1000)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(np.array([[2, 3], [5, 6]]))\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CIT6xUt6YZp",
        "outputId": "a6ff8861-0f74-4ca0-834e-814d8c65f597"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YLbfQJ796act"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}